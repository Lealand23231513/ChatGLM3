{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "from utils import *\n",
    "shutil.copy('1', Path(DEFAULT_CACHE_DIR+'/cached-files'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"chatglm3-6b\",\n",
    "    base_url=\"http://localhost:8765/v1/\",\n",
    "    api_key=\"EMPTY\"# type:ignore\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I help you today?')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 核心控制模块\n",
    "from typing import Mapping\n",
    "import openai\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from Retrieval_qa import retrieval_auto_runner\n",
    "from utils import *\n",
    "from websearch2 import *\n",
    "from websearch2 import get_customed_arxiv_search_tool\n",
    "from Retrieval_qa import get_retrieval_tool\n",
    "from global_var import get_global_value, set_global_value\n",
    "from langchain.agents import create_openai_functions_agent\n",
    "from langchain.agents import AgentExecutor, create_structured_chat_agent, load_tools\n",
    "# from ChatGLM3 import ChatGLM3\n",
    "from ChatGLM3Agent import *\n",
    "from typing import List\n",
    "from langchain.agents import Tool, AgentOutputParser\n",
    "from langchain.prompts import StringPromptTemplate\n",
    "\n",
    "logger = logging.getLogger('SDFDSFF')\n",
    "GLM3_TEMPLATE = \"You can answer using the tools, or answer directly using your knowledge without using the tools. Respond to the human as helpfully and accurately as possible.\\nYou have access to the following tools:\\n{tools}\\nUse a json blob to specify a tool by providing an action key (tool name) and an action_input key (tool input).\\nValid \\\"action\\\" values: \\\"Final Answer\\\" or  [{tool_names}]Provide only ONE action per $JSON_BLOB, as shown:\\n\\n```\\n{{{{\\n  \\\"action\\\": $TOOL_NAME,\\n  \\\"action_input\\\": $INPUT\\n}}}}\\n```\\n\\nFollow this format:\\n\\nQuestion: input question to answer\\nThought: consider previous and subsequent steps\\nAction:\\n```\\n$JSON_BLOB\\n```\\nObservation: action result\\n... (repeat Thought/Action/Observation N times)\\nThought: I know what to respond\\nAction:\\n```\\n{{{{\\n  \\\"action\\\": \\\"Final Answer\\\",\\n  \\\"action_input\\\": \\\"Final response to human\\\"\\n}}}}\\nBegin! Reminder to ALWAYS respond with a valid json blob of a single action. Use tools if necessary. Respond directly if appropriate. Format is Action:```$JSON_BLOB```then Observation:.\\nhistory: {history}\\n\\nQuestion: {input}\\n\\nThought: {agent_scratchpad}\"\n",
    "from langchain.agents import Tool, AgentOutputParser\n",
    "from langchain.prompts import StringPromptTemplate\n",
    "from typing import List\n",
    "from langchain.schema import AgentAction, AgentFinish\n",
    "class CustomPromptTemplate(StringPromptTemplate):\n",
    "    template: str\n",
    "    tools: List[BaseTool]\n",
    "\n",
    "    def format(self, **kwargs) -> str:\n",
    "        intermediate_steps = kwargs.pop(\"intermediate_steps\")\n",
    "        thoughts = \"\"\n",
    "        for action, observation in intermediate_steps:\n",
    "            thoughts += action.log\n",
    "            thoughts += f\"\\nObservation: {observation}\\nThought: \"\n",
    "        kwargs[\"agent_scratchpad\"] = thoughts\n",
    "        kwargs[\"tools\"] = \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in self.tools])\n",
    "        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in self.tools])\n",
    "        return self.template.format(**kwargs)\n",
    "\n",
    "class CustomOutputParser(AgentOutputParser):\n",
    "    begin: bool = False\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.begin = True\n",
    "\n",
    "    def parse(self, llm_output: str) -> AgentFinish | tuple[dict[str, str], str] | AgentAction:\n",
    "        if self.begin:\n",
    "            self.begin = False\n",
    "            stop_words = [\"Observation:\"]\n",
    "            min_index = len(llm_output)\n",
    "            for stop_word in stop_words:\n",
    "                index = llm_output.find(stop_word)\n",
    "                if index != -1 and index < min_index:\n",
    "                    min_index = index\n",
    "                llm_output = llm_output[:min_index]\n",
    "\n",
    "        if \"Final Answer:\" in llm_output:\n",
    "            self.begin = True\n",
    "            return AgentFinish(\n",
    "                return_values={\"output\": llm_output.split(\"Final Answer:\", 1)[-1].strip()},\n",
    "                log=llm_output,\n",
    "            )\n",
    "        parts = llm_output.split(\"Action:\")\n",
    "        if len(parts) < 2:\n",
    "            return AgentFinish(\n",
    "                return_values={\"output\": f\"调用agent工具失败，该回答为大模型自身能力的回答:\\n\\n `{llm_output}`\"},\n",
    "                log=llm_output,\n",
    "            )\n",
    "\n",
    "        action = parts[1].split(\"Action Input:\")[0].strip()\n",
    "        action_input = parts[1].split(\"Action Input:\")[1].strip()\n",
    "        try:\n",
    "            ans = AgentAction(\n",
    "                tool=action,\n",
    "                tool_input=action_input.strip(\" \").strip('\"'),\n",
    "                log=llm_output\n",
    "            )\n",
    "            return ans\n",
    "        except:\n",
    "            return AgentFinish(\n",
    "                return_values={\"output\": f\"调用agent失败: `{llm_output}`\"},\n",
    "                log=llm_output,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def call_agent(user_input:str, history:list[Mapping[str,str]]):\n",
    "    tools_inst = [get_customed_arxiv_search_tool(load_all_available_meta=True, download=True), get_retrieval_tool()]\n",
    "    endpoint_url = \"http://localhost:8765/v1/chat/completions\"\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"chatglm3-6b\",\n",
    "        base_url=\"http://localhost:8765/v1/\",\n",
    "        api_key=\"EMPTY\"# type:ignore\n",
    "    )\n",
    "    # prompt = hub.pull(\"hwchase17/structured-chat-agent\")\n",
    "    # prompt = CustomPromptTemplate(\n",
    "    #         template=GLM3_TEMPLATE,\n",
    "    #         tools=tools_inst,\n",
    "    #         input_variables=[\"input\", \"intermediate_steps\", \"history\"]\n",
    "    #     )\n",
    "    output_parser = CustomOutputParser()\n",
    "    agent_executor = initialize_glm3_agent(\n",
    "                llm=llm,\n",
    "                tools=tools_inst,\n",
    "                callback_manager=None,\n",
    "                prompt=GLM3_TEMPLATE,\n",
    "                input_variables=[\"input\", \"intermediate_steps\", \"history\"],\n",
    "                verbose=True,\n",
    "            )\n",
    "    agent_executor.invoke({\"input\": user_input, \"history\":[convert_dict_to_message(m) for m in history]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "call_agent('帮我找有关AI的论文',[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_inst = [get_customed_arxiv_search_tool(load_all_available_meta=True, download=True), get_retrieval_tool()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arxiv: A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.\n",
      "retrieval: Retrieve the content of the cached papers and answer questions.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in tools_inst]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can answer using the tools, or answer directly using your knowledge without using the tools. Respond to the human as helpfully and accurately as possible.\n",
      "You have access to the following tools:\n",
      "{tools}\n",
      "Use a json blob to specify a tool by providing an action key (tool name) and an action_input key (tool input).\n",
      "Valid \"action\" values: \"Final Answer\" or  [{tool_names}]Provide only ONE action per $JSON_BLOB, as shown:\n",
      "\n",
      "```\n",
      "{{{{\n",
      "  \"action\": $TOOL_NAME,\n",
      "  \"action_input\": $INPUT\n",
      "}}}}\n",
      "```\n",
      "\n",
      "Follow this format:\n",
      "\n",
      "Question: input question to answer\n",
      "Thought: consider previous and subsequent steps\n",
      "Action:\n",
      "```\n",
      "$JSON_BLOB\n",
      "```\n",
      "Observation: action result\n",
      "... (repeat Thought/Action/Observation N times)\n",
      "Thought: I know what to respond\n",
      "Action:\n",
      "```\n",
      "{{{{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"Final response to human\"\n",
      "}}}}\n",
      "Begin! Reminder to ALWAYS respond with a valid json blob of a single action. Use tools if necessary. Respond directly if appropriate. Format is Action:```$JSON_BLOB```then Observation:.\n",
      "history: {history}\n",
      "\n",
      "Question: {input}\n",
      "\n",
      "Thought: {agent_scratchpad}\n"
     ]
    }
   ],
   "source": [
    "print(GLM3_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StructuredGLM3ChatAgent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sciagent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
